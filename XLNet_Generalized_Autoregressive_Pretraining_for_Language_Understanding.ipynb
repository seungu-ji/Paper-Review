{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLNet: Generalized Autoregressive Pretraining for Language Understanding.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNNAVfPBd+7GOIIp3IzVwCI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viriditass/Paper-Review/blob/main/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VQtAwwlDlNQ"
      },
      "source": [
        "# **XLNet: Generalized Autoregressive Pretraining for Language Understanding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOCKTwfBDlQk"
      },
      "source": [
        "논문 링크: https://arxiv.org/abs/1906.08237"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6Nt-P2pDlTM"
      },
      "source": [
        "본 논문에서 제안하는 XLNet은 auto-regressive model(Ex. GPT)와 auto-encoder model(Ex. BERT)의 장점을 결합하여 만든 generalized AR pretraining model이다. 또한, 본 논문은 당시 NLP task에서 SOTA 성능을 보이는 BERT를 큰 차이로 outperform했다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifOXWufEDm7s"
      },
      "source": [
        "AR model의 단점은 BERT에서 살펴봤으니 생략하고 이번엔 BERT와 같은 AE model의 단점만 살펴보겠다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MehewgY5Dm99"
      },
      "source": [
        " AR과 달리 AE의 경우 특정 [MASK] token을 맞추기 위해  양방향 정보(Bidirectional self-attention)를 이용할 수 있다. 그러나, **independant assuumption으로 모든 [MASK] token이 독립적으로 예측됨으로 이들 사이 사이의 dependency를 학습할 수 없다.** 또한,**noise(=[MASK] token) 자체는 실제 fine-tuning 과정에서 등장하지 않아 pre-training과 fine-tuning 사이의 불일치를 야기한다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DblipFs3DnAz"
      },
      "source": [
        "이해를 돕기 위해 예시를 들어보자면 \"나의 고향은 전라남도 광주다\" 라는 문장에서 \"전라남도\"와 \"광주다\"를 [MASK] 처리하게 된다면, \"나의 고향은 MASK MASK\" 라는 문장이 되는데, \"나의 고향은\" 이라는 문맥만 보면 [MASK]를 예측하기 어렵지만, 전라남도를 고려하면 좀 더 맞추기 쉬워진다. 하지만, BERT의 경우 이런것들을 전혀 고려하지 않고 각각의 [MASK] 단어가 독립적이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m045bAP8Zdsu"
      },
      "source": [
        "Transformer-XL을 먼저 읽고 봐야겠다."
      ]
    }
  ]
}